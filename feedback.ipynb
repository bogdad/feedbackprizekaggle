{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc073b2-88e0-42e3-97c6-31659c49ce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file kfold_models already exists.\n",
      "A subdirectory or file fold_models already exists.\n",
      "A subdirectory or file models already exists.\n",
      "A subdirectory or file tar_models already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir kfold_models\n",
    "!mkdir fold_models\n",
    "!mkdir models\n",
    "!mkdir tar_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f83139c4-a093-4643-88b3-86a12aa81eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (4.15.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: filelock in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: six in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6262d02f-30e3-4f2b-be66-fa1f7486c8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!cp ../input/feedbackprizemodeldatasettarmodels/* tar_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a2e8b7-97a6-4c51-a6d7-2facc621d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamToLogger(object):\n",
    "    def __init__(self, queue):\n",
    "        self.queue = queue\n",
    "\n",
    "    def setold(self, old):\n",
    "        self.old = old\n",
    "        \n",
    "    def write(self, buf):\n",
    "        self.old.write(buf)\n",
    "        for line in buf.rstrip().splitlines():\n",
    "            self.queue.put(line.rstrip())\n",
    "    def flush(self):\n",
    "        self.old.flush()\n",
    "    def getq(self):\n",
    "        return self.queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9773d3a6-9566-4bab-bc33-6649a697c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callable_wrapper(stdoutl, stderrl, fun, *args, **kvargs):\n",
    "    import sys\n",
    "    stdoutl.setold(sys.stdout)\n",
    "    stderrl.setold(sys.stderr)\n",
    "    sys.stdout = stdoutl\n",
    "    sys.stderr = stderrl\n",
    "    def handle_exception(exc_type, exc_value, exc_traceback):\n",
    "        stdoutl.getq().put(\"XXXDONEXXX\")\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "    sys.excepthook = handle_exception\n",
    "    fun(*args, **kvargs)\n",
    "    stdoutl.getq().put(\"XXXDONEXXX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a620c672-f65d-44f6-bcfa-47d9ceb27211",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is https://github.com/philtrade/mpify\n",
    "## thanks!\n",
    "## for spawning a single process for gpu cuda computation\n",
    "## to help tear down gpu ram when finished.\n",
    "import os, sys, re\n",
    "from pathos.helpers import mp\n",
    "from typing import Callable\n",
    "from contextlib import AbstractContextManager, nullcontext\n",
    "#  - globals() doesn't necessarily return the '__main__' global scope when inside package function,\n",
    "#    thus we use sys.modules['__main__'].__dict__.\n",
    "\n",
    "def import_star(modules:[str], ns:dict=None):\n",
    "    \"\"\"Import ``*`` from a list of module, into namespace ns (default to '__main__')\n",
    "    Args:\n",
    "        modules: list of modules or packages\n",
    "        ns: destination namespace, optional. If not provided, will default to '__main__'\n",
    "    \"\"\"\n",
    "    global_imports([f\"from {m} import *\" for m in modules], ns)\n",
    "\n",
    "    \n",
    "def global_imports(imports:[str], ns:dict=None):\n",
    "    \"\"\"\n",
    "    Parse and execute multiple import statements, and import into target namespace 'ns'\n",
    "    Args:\n",
    "        imports: list of import statements, as in Python code.  Supported formats include:\n",
    "            * import x, y, z as z_alias\n",
    "            * from A import x\n",
    "            * from A import z as z_alias\n",
    "            * from A import x, y, z as z_alias\n",
    "            Not supported: 'from A import (a, b)'\n",
    "        ns: target namespace to import into.  Default to '__main__'\n",
    "    \"\"\"\n",
    "    import os, sys, re, multiprocess as mp\n",
    "    from typing import Callable\n",
    "    from contextlib import AbstractContextManager, nullcontext\n",
    "\n",
    "    if ns is None:\n",
    "        import sys\n",
    "        ns = sys.modules['__main__'].__dict__\n",
    "    pat = re.compile(r'^\\s*?(?:from\\s+?(\\S+?)\\s+?)?import\\s+?(.+)$')\n",
    "    pat_as = re.compile(r'^\\s*?(\\S+?)(?:\\s*?as\\s+?(\\S+?))?\\s*$')\n",
    "    for parsed in filter(lambda p:p,[pat.match(i) for i in imports]):\n",
    "        (from_, imp_)  = parsed.groups()\n",
    "        imps = imp_.split(',')\n",
    "\n",
    "        # Parse \"from X import ...\"\n",
    "        from_mod = __import__(from_, fromlist=['']) if from_ else None\n",
    "\n",
    "        for name in imps: # each comma-separated item in import a, b, x as y\n",
    "            (x, y) = pat_as.match(name).groups()\n",
    "            if y is None: y=x\n",
    "            if x == '*': # Handle starred import: 'from X import *'\n",
    "                assert from_, SyntaxError(f\"From what <module> are you trying to 'import *': {parsed.string}\")\n",
    "                importables = getattr(from_mod, \"__all__\", [n for n in dir(from_mod) if not n.startswith('_')])\n",
    "                for o in importables: ns[o] = getattr(from_mod, o)\n",
    "            else: # x is either a name in 1 module, OR a module itself\n",
    "                ns[y] = getattr(from_mod, x) if from_ else __import__(x, fromlist=[''])\n",
    "\n",
    "def _contextualize(i:int, nprocs:int, fn:Callable, cm:AbstractContextManager, l=None, env:dict={}, imports=\"\"):\n",
    "    \"Return a function that will setup os.environ and execute a target function within a context manager.\"\n",
    "    import os, sys, re, multiprocess as mp\n",
    "    from typing import Callable\n",
    "    from contextlib import AbstractContextManager, nullcontext\n",
    "\n",
    "    def global_imports(imports:[str], ns:dict=None):\n",
    "        \"\"\"\n",
    "        Parse and execute multiple import statements, and import into target namespace 'ns'\n",
    "        Args:\n",
    "            imports: list of import statements, as in Python code.  Supported formats include:\n",
    "                * import x, y, z as z_alias\n",
    "                * from A import x\n",
    "                * from A import z as z_alias\n",
    "                * from A import x, y, z as z_alias\n",
    "                Not supported: 'from A import (a, b)'\n",
    "            ns: target namespace to import into.  Default to '__main__'\n",
    "        \"\"\"\n",
    "\n",
    "        if ns is None:\n",
    "            import sys\n",
    "            ns = sys.modules['__main__'].__dict__\n",
    "        pat = re.compile(r'^\\s*?(?:from\\s+?(\\S+?)\\s+?)?import\\s+?(.+)$')\n",
    "        pat_as = re.compile(r'^\\s*?(\\S+?)(?:\\s*?as\\s+?(\\S+?))?\\s*$')\n",
    "        for parsed in filter(lambda p:p,[pat.match(i) for i in imports]):\n",
    "            (from_, imp_)  = parsed.groups()\n",
    "            imps = imp_.split(',')\n",
    "\n",
    "            # Parse \"from X import ...\"\n",
    "            from_mod = __import__(from_, fromlist=['']) if from_ else None\n",
    "\n",
    "            for name in imps: # each comma-separated item in import a, b, x as y\n",
    "                (x, y) = pat_as.match(name).groups()\n",
    "                if y is None: y=x\n",
    "                if x == '*': # Handle starred import: 'from X import *'\n",
    "                    assert from_, SyntaxError(f\"From what <module> are you trying to 'import *': {parsed.string}\")\n",
    "                    importables = getattr(from_mod, \"__all__\", [n for n in dir(from_mod) if not n.startswith('_')])\n",
    "                    for o in importables: ns[o] = getattr(from_mod, o)\n",
    "                else: # x is either a name in 1 module, OR a module itself\n",
    "                    ns[y] = getattr(from_mod, x) if from_ else __import__(x, fromlist=[''])\n",
    "\n",
    "    \n",
    "    if l: assert i < len(l), ValueError(\"Invalid index {i}, exceeds size of the result list: {len(l)}\")\n",
    "    def _cfn(*args, **kwargs):\n",
    "        import os\n",
    "        os.environ.update({\"LOCAL_RANK\":str(i), \"LOCAL_WORLD_SIZE\":str(nprocs)})\n",
    "        try:\n",
    "            import sys\n",
    "            # import env into '__main__', which can be in a subprocess here.\n",
    "            g = sys.modules['__main__'].__dict__\n",
    "            global_imports(imports.split('\\n'), g)\n",
    "            g.update(env)\n",
    "            with cm or nullcontext(): r = fn(*args, **kwargs)\n",
    "            if l: l[i] = r\n",
    "            return r\n",
    "        finally: map(lambda k: os.environ.pop(k, None), (\"LOCAL_RANK\", \"LOCAL_WORLD_SIZE\"))               \n",
    "    return _cfn\n",
    "\n",
    "def ranch(nprocs:int, fn:Callable, *args, caller_rank:int=0, gather:bool=True, ctx:AbstractContextManager=None, need:str=\"\", imports=\"\", **kwargs):\n",
    "    \"\"\" Execute `fn(\\*args, \\*\\*kwargs)` distributedly in `nprocs` processes.  User can\n",
    "    serialize over objects and functions, spell out import statements, manage execution\n",
    "    context, gather results, and the parent process can participate as one of the workers.\n",
    "    If `caller_rank` is `0 <= caller_rank < nprocs`, only `nprocs - 1` processes will be forked, and the caller process will be a worker to run its share of `fn(..)`.\n",
    "    If `caller_rank` is ``None``, `nprocs` processes will be forked.\n",
    "    Inside each worker process, its relative rank among all workers is set up in `os.environ['LOCAL_RANK']`, and the total\n",
    "    number of workers is set up in `os.environ['LOCAL_WORLD_SIZE']`, both as strings.\n",
    "    Then import statements in `imports`, followed by any objects/functions in `need`, are brought\n",
    "    into the python global namespace.\n",
    "    Then, context manager `ctx` is applied around the call `fn(\\*args, \\*\\*kwargs)`.\n",
    "    Return value of each worker can be gathered in a list (indexed by the process's rank)\n",
    "    and returned to the caller of `ranch()`.\n",
    "    Args:\n",
    "        nprocs: Number of processes to fork.  Visible as a string in `os.environ['LOCAL_WORLD_SIZE']`\n",
    "            in all worker processes.\n",
    "        fn: Function to execute on the worker pool\n",
    "        \\*args: Positional arguments by values to `fn(\\*args....)`\n",
    "        \\*\\*kwargs: Named parameters to `fn(x=..., y=....)`\n",
    "        caller_rank: Rank of the parent process.  ``0 <= caller_rank < nprocs`` to join, ``None`` to opt out. Default to ``0``.\n",
    "            In distributed data parallel, 0 means the leading process.\n",
    "        gather: if ``True``, `ranch` will return a list of return values from each worker, indexed by their ranks.\n",
    "            If ``False``, and if 'caller_rank' is not None (meaning parent process is a worker),\n",
    "            `ranch()` will return whatever the parent process' `fn(...)` returns.\n",
    "        ctx: User defined context manager to be used in a 'with'-clause around the 'fn(...)' call in worker processes.\n",
    "            Subclassed from AbstractContextManager, ctx needs to define '__enter__()' and '__exit__()' methods.\n",
    "        need: Space-separated names of objects/functions to be serialized over to the subprocesses.\n",
    "        imports: A multiline string of `import` statements to execute in the subprocesses\n",
    "            before `fn()` execution.  Supported formats:\n",
    "            * `import x, y, z as zoo`\n",
    "            * `from A import x`\n",
    "            * `from A import z as zoo`\n",
    "            * `from A import x, y, z as zoo`\n",
    "            * Not supported: `from A import (x, y)`\n",
    "    Returns:\n",
    "        ``None``, or list of results from worker processes, indexed by their `LOCAL_RANK`: ``[res_0, res_1, .... res_{nprocs-1}]``\n",
    "    \"\"\"\n",
    "\n",
    "    assert nprocs > 0, ValueError(\"nprocs: # of processes to launch must be > 0\")\n",
    "    manager = mp.Manager()\n",
    "    queue = manager.Queue()\n",
    "\n",
    "    children_ranks = list(range(nprocs))\n",
    "    if caller_rank is not None:\n",
    "        assert 0 <= caller_rank < nprocs, ValueError(f\"Invalid caller_rank {caller_rank}, must satisfy 0 <= caller_rank < {nprocs}\")\n",
    "        children_ranks.pop(caller_rank)\n",
    "    multiproc_ctx, procs = mp.get_context(\"spawn\"), []\n",
    "    result_list = multiproc_ctx.Manager().list([None] * nprocs) if gather else None\n",
    "    try:\n",
    "        # pass globals in this process to subprocess via fn's wrapper, 'target_fn'\n",
    "        env = {k : sys.modules['__main__'].__dict__[k] for k in need.split()}\n",
    "        for rank in children_ranks:\n",
    "            target_fn = _contextualize(rank, nprocs, fn, cm=ctx, l=result_list, env=env, imports=imports)\n",
    "            sle = StreamToLogger(queue)\n",
    "            slo = StreamToLogger(queue)\n",
    "            p = multiproc_ctx.Process(target=callable_wrapper, args=[sle, slo, target_fn, *args], kwargs=kwargs)\n",
    "            procs.append(p)\n",
    "            p.start()\n",
    "        p_res = (_contextualize(caller_rank, nprocs, fn, cm=ctx, l=result_list, env=env, imports=imports))(*args, **kwargs) if caller_rank is not None else None\n",
    "        while True:\n",
    "            mess = queue.get()\n",
    "            if mess==\"XXXDONEXXX\":\n",
    "                break\n",
    "            print(mess)\n",
    "        for p in procs: p.join()\n",
    "        return result_list if gather else p_res\n",
    "    finally:\n",
    "        for p in procs: p.terminate(), p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00a6e814-fcf3-4aa8-9f33-fc19310af3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_model_one_inproc(model_fold_storage, model_type, i_fold, train_idx, valid_idx, X, y, Xtest):\n",
    "    import os\n",
    "    print(\"kfold {0}\".format(str(os.getpid())))\n",
    "    import tensorflow as tf\n",
    "    from keras import backend as K\n",
    "    import gc\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    import pprint\n",
    "    Xt = X.iloc[train_idx]\n",
    "    yt = y[train_idx]\n",
    "    Xv = X.iloc[valid_idx]\n",
    "    yv = y[valid_idx]\n",
    "\n",
    "    model, model_files = model_fold_storage.try_load_model(model_type, i_fold)\n",
    "    if model is None:\n",
    "        print(\"creating model {0}\".format(model_type.name()))\n",
    "        model = model_type.create()\n",
    "        print(\"fitting fold {0}\".format(i_fold))\n",
    "        model_type.fit(model, Xt, yt, Xv, yv)\n",
    "        model_files = model_fold_storage.save_model(model_type, i_fold, model)\n",
    "\n",
    "    global oof_df_fold\n",
    "    print(\"predicting oof fold {0}\".format(i_fold))\n",
    "    oof_df_fold = model_type.predict(model, Xv)\n",
    "    global metrics\n",
    "    print(\"evaluating fold {0}\".format(i_fold))\n",
    "    metrics = model_type.evaluate(model, Xv, yv)\n",
    "    pretty_metrics = pprint.pformat(metrics)\n",
    "    print(\"fold {0} metrics {1}\".format(i_fold, pretty_metrics))\n",
    "    global test_df_fold\n",
    "    test_df_fold = model_type.predict(model, Xtest)\n",
    "    model_fold_storage.save_fold(model_type, X, y, valid_idx, train_idx, i_fold, oof_df_fold, test_df_fold, metrics)\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    return oof_df_fold,test_df_fold,metrics, model_files\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f1ec41-ecf0-41c8-a316-ce90f5e29bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_model_inproc(model_fold_storage, problem_type, model_type, X, y, Xtest):\n",
    "    import os\n",
    "    print(\"kfold {0}\".format(str(os.getpid())))\n",
    "    import tensorflow as tf\n",
    "    from keras import backend as K\n",
    "    import gc\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    split = 5\n",
    "    global pred_dim\n",
    "    global pred_dim_list\n",
    "    pred_dim_list = [i if i>0 else X.shape[0] for i in problem_type.pred_dim()]\n",
    "    pred_dim = tuple(i for i in pred_dim_list)\n",
    "    global oof_pred\n",
    "    oof_pred = np.zeros(pred_dim)\n",
    "    global test_pred\n",
    "    test_pred_dim_list = [i if i>0 else Xtest.shape[0] for i in problem_type.pred_dim()]\n",
    "    test_pred_dim = tuple(i for i in test_pred_dim_list)\n",
    "    test_pred = np.zeros((split,) + test_pred_dim)\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    kf = StratifiedKFold(n_splits=split, shuffle=True)\n",
    "    i_fold=0\n",
    "    all_train_idx = []\n",
    "    all_valid_idx = []\n",
    "    all_model_files = []\n",
    "    global all_metrics\n",
    "    all_metrics = {}\n",
    "    global train_idx\n",
    "    global valid_idx\n",
    "    for train_idx, valid_idx in kf.split(X, y.argmax(1)):\n",
    "        all_train_idx.append(train_idx)\n",
    "        all_valid_idx.append(valid_idx)\n",
    "    nfolds = len(all_valid_idx)\n",
    "    for i_fold in tqdm(range(nfolds), mininterval=5):\n",
    "        print(\"starting {0} fold\".format(i_fold))\n",
    "        train_idx = all_train_idx[i_fold]\n",
    "        valid_idx = all_valid_idx[i_fold]\n",
    "        \n",
    "        oof_df_fold,test_df_fold,metrics, model_files = kfold_model_one(model_fold_storage, model_type, i_fold, train_idx, valid_idx, X, y, Xtest)\n",
    "        \n",
    "        for key in metrics:\n",
    "            if key in all_metrics:\n",
    "                all_metrics[key]=np.hstack((all_metrics[key], metrics[key]))\n",
    "            else: \n",
    "                all_metrics[key]=metrics[key]\n",
    "        oof_pred[valid_idx, :] = np.reshape(oof_df_fold, oof_df_fold.shape)\n",
    "        test_pred[i_fold]=test_df_fold\n",
    "        all_model_files.append(model_files)\n",
    "        i_fold+=1\n",
    "        \n",
    "    model_fold_storage.save_kfold(model_type, X, y, all_valid_idx, all_train_idx, all_model_files, oof_pred, test_pred, all_metrics)\n",
    "    return oof_pred, test_pred, all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336fb098-6db7-4582-bf30-13bd10e8e892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8350b36e-d7fd-48ec-9edb-bf7620abf27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file kfold_models already exists.\n",
      "A subdirectory or file fold_models already exists.\n",
      "A subdirectory or file models already exists.\n",
      "A subdirectory or file tar_models already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir kfold_models\n",
    "!mkdir fold_models\n",
    "!mkdir models\n",
    "!mkdir tar_models\n",
    "class ModelFoldStorage:\n",
    "    def save_fold(self, model_type, X, y, valid_idx, train_idx, i_fold, oof_df_fold, test_df_fold, metrics):\n",
    "        import pickle\n",
    "        foldData = {}\n",
    "        foldData['X'], foldData['y'],foldData['valid_idx'],foldData['train_idx']=X,y,valid_idx,train_idx\n",
    "        foldData['oof_df_fold'],foldData['i_fold'], foldData['metrics'], foldData['test_df_fold']=oof_df_fold,i_fold, metrics, test_df_fold\n",
    "        with open(r\"fold_models/\"+model_type.name()+\"_\"+str(i_fold)+\"_fold.bin\", \"wb\") as output_file:\n",
    "            pickle.dump(foldData, output_file)\n",
    "    def try_load_kfold(self, model_type):\n",
    "        import pickle\n",
    "        with open(r\"kfold_models/\"+model_type.name()+\"_kfold.bin\", \"rb\") as input_file:\n",
    "            modelTypeData = pickle.read(input_file)\n",
    "            return modelTypeData['oof_pred'], modelTypeData['test_pred'], modelTypeData['all_metrics']\n",
    "        return None\n",
    "    def save_kfold(self, model_type, X, y, all_valid_idx, all_train_idx, all_model_files, oof_pred, test_pred, all_metrics):\n",
    "        import pickle\n",
    "        modelTypeData = {}\n",
    "        modelTypeData['X'], modelTypeData['y'],modelTypeData['all_valid_idx'],modelTypeData['all_train_idx'], modelTypeData['all_model_files']=X,y,all_valid_idx,all_train_idx, all_model_files\n",
    "        modelTypeData['model_type'], modelTypeData['oof_pred'],modelTypeData['all_metrics'], modelTypeData['test_pred']=model_type, oof_pred, all_metrics, test_pred\n",
    "        with open(r\"kfold_models/\"+model_type.name()+\"_kfold.bin\", \"wb\") as output_file:\n",
    "            pickle.dump(modelTypeData, output_file)\n",
    "    def save_model(self, model_type, i_fold, model):\n",
    "        file = r\"models/\"+model_type.name()+\"_\"+str(i_fold)+\"_model\"\n",
    "        files = model_type.save(file, model)\n",
    "        files = self.tar(file, files)\n",
    "        print(\"saved model %s to %s (%s)\" % (model_type.name(), file, \", \".join(files)))\n",
    "        return files\n",
    "    def try_load_model(self, model_type, i_fold):\n",
    "        file = r\"models/\"+model_type.name()+\"_\"+str(i_fold)+\"_model\"\n",
    "        file = self.try_untar(file)\n",
    "        model, model_files = model_type.load(file)\n",
    "        if model is None:\n",
    "            print(\"could not load model %s from %s\" % (model_type.name(), file))\n",
    "        else:\n",
    "            print(\"loaded model %s from %s\" % (model_type.name(), file))\n",
    "        return model, model_files\n",
    "    def tar(self, file, files):\n",
    "        import tarfile\n",
    "        import os\n",
    "        tar = \"tar_\"+file + \".tar.gz.xyz\"\n",
    "        with tarfile.open(tar, \"w:gz\") as tara:\n",
    "            for f in files:\n",
    "                tara.add(f, arcname=os.path.basename(f))\n",
    "        files.append(tar)\n",
    "        return files\n",
    "    def try_untar(self, file):\n",
    "        import pathlib\n",
    "        import tensorflow as tf\n",
    "        import os\n",
    "        import tarfile\n",
    "        file_gz = \"tar_\"+file + \".tar.gz.xyz\"\n",
    "        filepath = pathlib.PurePath(file)\n",
    "        file_name = filepath.name\n",
    "        if not os.path.exists(file) and os.path.exists(file_gz):\n",
    "            import atexit, shutil, tempfile\n",
    "            models_dir = tempfile.mkdtemp()\n",
    "            atexit.register(shutil.rmtree, models_dir)\n",
    "            if not models_dir.endswith('/'):\n",
    "                models_dir = models_dir + '/'\n",
    "            print(\"loading from\", file_gz)\n",
    "            target_model_name = models_dir + file_name\n",
    "            with tarfile.open(file_gz) as my_tar:\n",
    "                my_tar.extractall(models_dir) # specify which folder to extract to\n",
    "                my_tar.close()\n",
    "            file = target_model_name\n",
    "            return file\n",
    "        return file\n",
    "    \n",
    "model_fold_storage = ModelFoldStorage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7cf4571-97f1-4e9e-adf6-5d047c41865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyProblem:\n",
    "    def pred_dim(self):\n",
    "        return [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23db498-ecd7-4a79-a557-69c4c5c96752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DatasetMapFunction(input_ids, attn_masks, labels):\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attn_masks\n",
    "            }, labels\n",
    "class MyBertModel:\n",
    "    def __init__(self, debug=False):\n",
    "        self.debug = debug\n",
    "    def create(self):\n",
    "        import tensorflow as tf\n",
    "        import tensorflow_hub as hub\n",
    "        import tensorflow_text as text\n",
    "        from transformers import TFBertModel\n",
    "        model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
    "        attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
    "\n",
    "        bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
    "        bert_embds.trainable=False\n",
    "\n",
    "        intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
    "        output_layer = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n",
    "\n",
    "        discourse_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "        print(discourse_model.summary())\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        discourse_model.compile(optimizer=Adam(learning_rate=1e-5, decay=1e-6), \n",
    "                        loss='categorical_crossentropy', \n",
    "                        metrics=['accuracy'])\n",
    "        from transformers import BertTokenizerFast\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        return (discourse_model, self, model, tokenizer)\n",
    "    def load(self, file):\n",
    "        import os\n",
    "        import tensorflow as tf\n",
    "        import pickle\n",
    "        ttt1=file\n",
    "        ttt2=file+\"_bert\"\n",
    "        ttt3=file+\"_python\"\n",
    "        ttt4=file+\"_tokenizer\"\n",
    "        if not os.path.exists(ttt1) or not os.path.exists(ttt2) or not os.path.exists(ttt3) or not os.path.exists(ttt4):\n",
    "            return None, None\n",
    "        model_0 = tf.keras.models.load_model(ttt1)\n",
    "        model_2 = tf.keras.models.load_model(ttt2)\n",
    "        #with open(ttt3, \"rb\") as input_file:\n",
    "        #    model1_dict = pickle.load(input_file)\n",
    "        #self.__dict__.update(model1_dict)\n",
    "        from transformers import BertTokenizerFast\n",
    "        tokenizer_3 = BertTokenizerFast.from_pretrained(ttt4) \n",
    "        model_files = [ttt1, ttt2, ttt3, ttt4]\n",
    "        return (model_0, self, model_2, tokenizer_3), model_files\n",
    "    def save(self, file, model):\n",
    "        import shutil\n",
    "        import os\n",
    "        import tensorflow as tf\n",
    "        import pickle\n",
    "        ttt1=file\n",
    "        tf.keras.models.save_model(model[0], ttt1)\n",
    "        ttt2=file+\"_bert\"\n",
    "        tf.keras.models.save_model(model[2], ttt2)\n",
    "        ttt3=file+\"_python\"\n",
    "        with open(ttt3, \"wb\") as output_file:\n",
    "            pickle.dump(model[1].__dict__, output_file)\n",
    "        ttt4=file+\"_tokenizer\"\n",
    "        model[3].save_pretrained(ttt4)\n",
    "        return [ttt1, ttt2, ttt3, ttt4]\n",
    "    def name(self):\n",
    "        return \"bert\"\n",
    "    def fit(self, model, Xt, yt, Xv, yv):\n",
    "        X_input_ids, X_attn_masks, yt = self._transform(model, Xt, yt)\n",
    "        import tensorflow as tf\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, yt))\n",
    "        dataset = dataset.map(DatasetMapFunction)     # converting to required format for tensorflow dataset\n",
    "        dataset = dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor\n",
    "        X_val_input_ids, X_val_attn_masks, yv = self._transform(model, Xv, yv)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val_input_ids, X_val_attn_masks, yv))\n",
    "        val_dataset = val_dataset.map(DatasetMapFunction)     # converting to required format for tensorflow dataset\n",
    "        val_dataset = val_dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor\n",
    "        epochs = 5\n",
    "        if self.debug:\n",
    "            epochs=1\n",
    "        model[1].history = model[0].fit(dataset,\n",
    "            steps_per_epoch=200,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs, verbose=0)\n",
    "    def predict(self, model, X):\n",
    "        X_test_input_ids, X_test_attn_masks, _y = self._transform(model, X, None)\n",
    "        labels = model[0].predict([X_test_input_ids, X_test_attn_masks], verbose=0)\n",
    "        return labels\n",
    "    def evaluate(self, model, x, y):\n",
    "        import tensorflow as tf\n",
    "        X_input_ids, X_attn_masks, y = self._transform(model, x,y)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, y))\n",
    "        dataset = dataset.map(DatasetMapFunction)\n",
    "        dataset = dataset.shuffle(10000).batch(16, drop_remainder=True)\n",
    "        return model[0].evaluate(dataset, return_dict=True, verbose=0)\n",
    "    def _encode_data(self, df, ids, masks, tokenizer):\n",
    "        from tqdm.auto import tqdm\n",
    "        for i, text in tqdm(enumerate(df['text']), mininterval=5):\n",
    "            tokenized_text = tokenizer.encode_plus(\n",
    "                text,\n",
    "                max_length=256, \n",
    "                truncation=True, \n",
    "                padding='max_length', \n",
    "                add_special_tokens=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            ids[i, :] = tokenized_text.input_ids\n",
    "            masks[i, :] = tokenized_text.attention_mask\n",
    "        return ids, masks\n",
    "    \n",
    "    def _transform(self, model, X,y):\n",
    "        tokenizer = model[3]\n",
    "        X[\"text\"]=X['discourse_type'] + tokenizer.sep_token+ X['text']\n",
    "        if self.debug:\n",
    "            X = X.head(1000)\n",
    "            if y is not None:\n",
    "                y = y[0:1000,:]\n",
    "        \n",
    "        import numpy as np\n",
    "        X_input_ids = np.zeros((len(X), 256))\n",
    "        X_attn_masks = np.zeros((len(X), 256))\n",
    "        X_input_ids, X_attn_masks = self._encode_data(X, X_input_ids, X_attn_masks, tokenizer)\n",
    "        return X_input_ids, X_attn_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c169d1a4-a1ab-45bf-8957-489d2c45b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRobertaModel:\n",
    "    def __init__(self, debug=False):\n",
    "        self.debug = debug\n",
    "    def create(self):\n",
    "        import tensorflow as tf\n",
    "        import tensorflow_hub as hub\n",
    "        import tensorflow_text as text\n",
    "        from transformers import TFRobertaModel\n",
    "        model = TFRobertaModel.from_pretrained(\"roberta-base\")\n",
    "        input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
    "        attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
    "\n",
    "        bert_embds = model.roberta(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
    "        bert_embds.trainable=False\n",
    "\n",
    "        intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
    "        output_layer = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n",
    "\n",
    "        discourse_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "        print(discourse_model.summary())\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        discourse_model.compile(optimizer=Adam(learning_rate=1e-5, decay=1e-6), \n",
    "                        loss='categorical_crossentropy', \n",
    "                        metrics=['accuracy'])\n",
    "        from transformers import RobertaTokenizerFast\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "        return (discourse_model, self, model, tokenizer)\n",
    "    def load(self, file):\n",
    "        import os\n",
    "        import tensorflow as tf\n",
    "        import pickle\n",
    "        ttt1=file\n",
    "        ttt2=file+\"_bert\"\n",
    "        ttt3=file+\"_python\"\n",
    "        ttt4=file+\"_tokenizer\"\n",
    "        if not os.path.exists(ttt1) or not os.path.exists(ttt2) or not os.path.exists(ttt3) or not os.path.exists(ttt4):\n",
    "            return None, None\n",
    "        model_0 = tf.keras.models.load_model(ttt1)\n",
    "        model_2 = tf.keras.models.load_model(ttt2)\n",
    "        #with open(ttt3, \"rb\") as input_file:\n",
    "        #    model1_dict = pickle.load(input_file)\n",
    "        #self.__dict__.update(model1_dict)\n",
    "        from transformers import RobertaTokenizerFast\n",
    "        tokenizer_3 = RobertaTokenizerFast.from_pretrained(ttt4) \n",
    "        model_files = [ttt1, ttt2, ttt3, ttt4]\n",
    "        return (model_0, self, model_2, tokenizer_3), model_files\n",
    "    def save(self, file, model):\n",
    "        import shutil\n",
    "        import os\n",
    "        import tensorflow as tf\n",
    "        import pickle\n",
    "        ttt1=file\n",
    "        tf.keras.models.save_model(model[0], ttt1)\n",
    "        ttt2=file+\"_bert\"\n",
    "        tf.keras.models.save_model(model[2], ttt2)\n",
    "        ttt3=file+\"_python\"\n",
    "        with open(ttt3, \"wb\") as output_file:\n",
    "            pickle.dump(model[1].__dict__, output_file)\n",
    "        ttt4=file+\"_tokenizer\"\n",
    "        model[3].save_pretrained(ttt4)\n",
    "        return [ttt1, ttt2, ttt3, ttt4]\n",
    "    def name(self):\n",
    "        return \"roberta\"\n",
    "    def fit(self, model, Xt, yt, Xv, yv):\n",
    "        X_input_ids, X_attn_masks, yt = self._transform(model, Xt, yt)\n",
    "        import tensorflow as tf\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, yt))\n",
    "        dataset = dataset.map(DatasetMapFunction)     # converting to required format for tensorflow dataset\n",
    "        dataset = dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor\n",
    "        X_val_input_ids, X_val_attn_masks, yv = self._transform(model, Xv, yv)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val_input_ids, X_val_attn_masks, yv))\n",
    "        val_dataset = val_dataset.map(DatasetMapFunction)     # converting to required format for tensorflow dataset\n",
    "        val_dataset = val_dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor\n",
    "        epochs = 5\n",
    "        if self.debug:\n",
    "            epochs=1\n",
    "        model[1].history = model[0].fit(dataset,\n",
    "            steps_per_epoch=200,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs, verbose=0)\n",
    "    def predict(self, model, X):\n",
    "        X_test_input_ids, X_test_attn_masks, _y = self._transform(model, X, None)\n",
    "        labels = model[0].predict([X_test_input_ids, X_test_attn_masks], verbose=0)\n",
    "        return labels\n",
    "    def evaluate(self, model, x, y):\n",
    "        import tensorflow as tf\n",
    "        X_input_ids, X_attn_masks, y = self._transform(model, x,y)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, y))\n",
    "        dataset = dataset.map(DatasetMapFunction)\n",
    "        dataset = dataset.shuffle(10000).batch(16, drop_remainder=True)\n",
    "        return model[0].evaluate(dataset, return_dict=True, verbose=0)\n",
    "    def _encode_data(self, df, ids, masks, tokenizer):\n",
    "        from tqdm.auto import tqdm\n",
    "        for i, text in tqdm(enumerate(df['text']), mininterval=5):\n",
    "            tokenized_text = tokenizer.encode_plus(\n",
    "                text,\n",
    "                max_length=256, \n",
    "                truncation=True, \n",
    "                padding='max_length', \n",
    "                add_special_tokens=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            ids[i, :] = tokenized_text.input_ids\n",
    "            masks[i, :] = tokenized_text.attention_mask\n",
    "        return ids, masks\n",
    "    \n",
    "    def _transform(self, model, X,y):\n",
    "        tokenizer = model[3]\n",
    "        X[\"text\"]=X['discourse_type'] + tokenizer.sep_token+ X['text']\n",
    "        if self.debug:\n",
    "            X = X.head(1000)\n",
    "            if y is not None:\n",
    "                y = y[0:1000,:]\n",
    "        \n",
    "        import numpy as np\n",
    "        X_input_ids = np.zeros((len(X), 256))\n",
    "        X_attn_masks = np.zeros((len(X), 256))\n",
    "        X_input_ids, X_attn_masks = self._encode_data(X, X_input_ids, X_attn_masks, tokenizer)\n",
    "        return X_input_ids, X_attn_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8024c639-2615-43a2-a66c-d29ab764b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackProblem(MyProblem):\n",
    "    def pred_dim(self):\n",
    "        return [0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dce8e5d4-23aa-4d7c-8f53-966d0ed64381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', 2550)\n",
    "df_train = pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\n",
    "df_train[\"text\"] = df_train[\"essay_id\"].apply(lambda x: open(f'../input/feedback-prize-effectiveness/train/{x}.txt').read())\n",
    "df_test[\"text\"] = df_test[\"essay_id\"].apply(lambda x: open(f'../input/feedback-prize-effectiveness/test/{x}.txt').read())\n",
    "effectiveness_map = {\"Ineffective\":0, \"Adequate\":1,\"Effective\":2}\n",
    "df_train[\"target\"] = df_train[\"discourse_effectiveness\"].map(effectiveness_map)\n",
    "labels = np.zeros((len(df_train), 3))\n",
    "labels[np.arange(len(df_train)), df_train['target'].values] = 1\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f1961fd-1c66-424c-ac8a-f8190d345105",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_type = FeedbackProblem()\n",
    "model_type = MyRobertaModel(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd1669e5-d218-4ece-abcc-154b490940a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_all_functions():\n",
    "import inspect\n",
    "all_funcs = [tpl[0] for tpl in inspect.getmembers(sys.modules['__main__'], inspect.isfunction)]\n",
    "all_classes = [tpl[0] for tpl in inspect.getmembers(sys.modules['__main__'], inspect.isclass)]\n",
    "some_vars = ['model_fold_storage']\n",
    "some_funcs=[]\n",
    "some_classes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35373358-66d6-46ad-b828-a61e08641dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DatasetMapFunction _contextualize callable_wrapper global_imports import_star kfold_model_inproc kfold_model_one_inproc ranch AbstractContextManager FeedbackProblem ModelFoldStorage MyBertModel MyProblem MyRobertaModel StreamToLogger nullcontext model_fold_storage'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_names = []\n",
    "all_names.extend(all_funcs)\n",
    "all_names.extend(some_funcs)\n",
    "all_names.extend(all_classes)\n",
    "all_names.extend(some_classes)\n",
    "all_names.extend(some_vars)\n",
    "all_names=\" \".join(all_names)                             \n",
    "all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "834d6382-f8e3-4694-84ba-ea68e226e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imports = '''\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57761576-d269-4333-94da-0c6b75c2ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathos.helpers import mp\n",
    "import dill as pickle\n",
    "\n",
    "def kfold_model_mp(model_fold_storage, problem_type, model_type, X, y, Xtest):\n",
    "    res = ranch(1, kfold_model_inproc,model_fold_storage, problem_type, model_type, X, y, Xtest, \n",
    "                caller_rank=None, need=all_names, imports = all_imports)\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aa27f97-3faa-427e-92c8-e6be1edc8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathos.helpers import mp\n",
    "import dill as pickle\n",
    "\n",
    "def kfold_model_one(model_fold_storage, model_type, i_fold, train_idx, valid_idx, X, y, Xtest):\n",
    "    res = ranch(1, kfold_model_one_inproc, model_fold_storage, model_type, i_fold, train_idx, valid_idx, X, y, Xtest, \n",
    "                caller_rank=None, need=all_names, imports = all_imports)\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faf7c83d-c830-4ebf-8587-d969517713eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oof_pred, test_pred, all_metrics=kfold_model(env_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebd0bb-f354-496b-988a-9770ae56d24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfold 15944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting 0 fold\n",
      "kfold 14628\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "loaded model roberta from models/roberta_0_model\n",
      "predicting oof fold 0\n",
      "C:\\Users\\Vladimir\\AppData\\Local\\Temp/ipykernel_15944/398137073.py:109: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "4006it [00:05, 801.13it/s]\n",
      "\n",
      "7353it [00:09, 776.09it/s]\n"
     ]
    }
   ],
   "source": [
    "oof_pred, test_pred, all_metrics=kfold_model_inproc(model_fold_storage, problem_type, model_type, df_train, labels, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9303f89d-91b7-418a-8443-0d2ae12013c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.mean(test_pred, axis = 0)\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9047398-0053-4328-8e4b-b020f7131418",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../input/feedback-prize-effectiveness/sample_submission.csv')\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d28522-22bc-4cd1-8a1c-7f3721c0f6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b15325-5cb8-4a64-94b3-d60d486af151",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['discourse_id'] = df_test['discourse_id']\n",
    "sample_submission['Ineffective'] = pred_labels[:,0]\n",
    "sample_submission['Adequate'] = pred_labels[:,1]\n",
    "sample_submission['Effective'] = pred_labels[:,2]\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056337f-ab5b-4b33-9149-c08e5f85be88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4b53e-dd04-4aee-bf12-24c1a100e405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597397cf-a6ba-45f6-a41c-7f39177a26a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610cb68-06cb-4976-9491-72ac2acb128d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a1d66-1fe6-4267-8ee3-a0661ca9ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dat = []\n",
    "with open(\"kfold_models/bert_kfold.bin\", \"rb\") as f:\n",
    "    dat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbcc1f-8718-481f-9439-a5ee3346a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['all_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ff6cb-7afe-4ee6-a2f7-e99d08bfab6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587dd81e-441c-4839-b96d-71f72b40792e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
