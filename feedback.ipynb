{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bc073b2-88e0-42e3-97c6-31659c49ce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file kfold_models already exists.\n",
      "A subdirectory or file fold_models already exists.\n",
      "A subdirectory or file models already exists.\n",
      "A subdirectory or file tar_models already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir kfold_models\n",
    "!mkdir fold_models\n",
    "!mkdir models\n",
    "!mkdir tar_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f83139c4-a093-4643-88b3-86a12aa81eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\vladimir\\miniconda3\\lib\\site-packages (from sacremoses->transformers) (8.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6262d02f-30e3-4f2b-be66-fa1f7486c8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!cp ../input/feedbackprizemodeldatasettarmodels/* tar_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f1ec41-ecf0-41c8-a316-ce90f5e29bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_model_inproc(model_fold_storage, problem_type, model_type, X, y, Xtest):\n",
    "    import os\n",
    "    print(\"kfold\", str(os.getpid()))\n",
    "    import tensorflow as tf\n",
    "    from keras import backend as K\n",
    "    import gc\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    split = 5\n",
    "    global pred_dim\n",
    "    global pred_dim_list\n",
    "    pred_dim_list = [i if i>0 else X.shape[0] for i in problem_type.pred_dim()]\n",
    "    pred_dim = tuple(i for i in pred_dim_list)\n",
    "    global oof_pred\n",
    "    oof_pred = np.zeros(pred_dim)\n",
    "    global test_pred\n",
    "    test_pred_dim_list = [i if i>0 else Xtest.shape[0] for i in problem_type.pred_dim()]\n",
    "    test_pred_dim = tuple(i for i in test_pred_dim_list)\n",
    "    test_pred = np.zeros((split,) + test_pred_dim)\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    kf = StratifiedKFold(n_splits=split, shuffle=True)\n",
    "    i_fold=0\n",
    "    all_train_idx = []\n",
    "    all_valid_idx = []\n",
    "    all_model_files = []\n",
    "    global all_metrics\n",
    "    all_metrics = {}\n",
    "    global train_idx\n",
    "    global valid_idx\n",
    "    for train_idx, valid_idx in tqdm(kf.split(X, y.argmax(1))):\n",
    "        all_train_idx.append(train_idx)\n",
    "        all_valid_idx.append(valid_idx)\n",
    "    nfolds = len(all_valid_idx)\n",
    "    for i_fold in range(nfolds):\n",
    "        train_idx = all_train_idx[i_fold]\n",
    "        valid_idx = all_valid_idx[i_fold]\n",
    "        Xt = X.iloc[train_idx]\n",
    "        yt = y[train_idx]\n",
    "        Xv = X.iloc[valid_idx]\n",
    "        yv = y[valid_idx]\n",
    "        \n",
    "        model, model_files = model_fold_storage.try_load_model(model_type, i_fold)\n",
    "        if model is None:\n",
    "            model = model_type.create()\n",
    "            model_type.fit(model, Xt, yt, Xv, yv)\n",
    "            model_files = model_fold_storage.save_model(model_type, i_fold, model)\n",
    "        \n",
    "        global oof_df_fold\n",
    "        oof_df_fold = model_type.predict(model, Xv)\n",
    "        global metrics\n",
    "        metrics = model_type.evaluate(model, Xv, yv)\n",
    "        for key in metrics:\n",
    "            if key in all_metrics:\n",
    "                all_metrics[key]=np.hstack((all_metrics[key], metrics[key]))\n",
    "            else: \n",
    "                all_metrics[key]=metrics[key]\n",
    "        global test_df_fold\n",
    "        test_df_fold = model_type.predict(model, Xtest)\n",
    "        model_fold_storage.save_fold(model_type, X, y, valid_idx, train_idx, i_fold, oof_df_fold, test_df_fold, metrics)\n",
    "        \n",
    "        oof_pred[valid_idx, :] = np.reshape(oof_df_fold, oof_df_fold.shape)\n",
    "        test_pred[i_fold]=test_df_fold\n",
    "        all_model_files.append(model_files)\n",
    "        i_fold+=1\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    model_fold_storage.save_kfold(model_type, X, y, all_valid_idx, all_train_idx, all_model_files, oof_pred, test_pred, all_metrics)\n",
    "    return oof_pred, test_pred, all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8350b36e-d7fd-48ec-9edb-bf7620abf27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file kfold_models already exists.\n",
      "A subdirectory or file fold_models already exists.\n",
      "A subdirectory or file models already exists.\n",
      "A subdirectory or file tar_models already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir kfold_models\n",
    "!mkdir fold_models\n",
    "!mkdir models\n",
    "!mkdir tar_models\n",
    "class ModelFoldStorage:\n",
    "    def save_fold(self, model_type, X, y, valid_idx, train_idx, i_fold, oof_df_fold, test_df_fold, metrics):\n",
    "        import pickle\n",
    "        foldData = {}\n",
    "        foldData['X'], foldData['y'],foldData['valid_idx'],foldData['train_idx']=X,y,valid_idx,train_idx\n",
    "        foldData['oof_df_fold'],foldData['i_fold'], foldData['metrics'], foldData['test_df_fold']=oof_df_fold,i_fold, metrics, test_df_fold\n",
    "        with open(r\"fold_models/\"+model_type.name()+\"_\"+str(i_fold)+\"_fold.bin\", \"wb\") as output_file:\n",
    "            pickle.dump(foldData, output_file)\n",
    "    def try_load_kfold(self, model_type):\n",
    "        import pickle\n",
    "        with open(r\"kfold_models/\"+model_type.name()+\"_kfold.bin\", \"rb\") as input_file:\n",
    "            modelTypeData = pickle.read(input_file)\n",
    "            return modelTypeData['oof_pred'], modelTypeData['test_pred'], modelTypeData['all_metrics']\n",
    "        return None\n",
    "    def save_kfold(self, model_type, X, y, all_valid_idx, all_train_idx, all_model_files, oof_pred, test_pred, all_metrics):\n",
    "        import pickle\n",
    "        modelTypeData = {}\n",
    "        modelTypeData['X'], modelTypeData['y'],modelTypeData['all_valid_idx'],modelTypeData['all_train_idx'], modelTypeData['all_model_files']=X,y,all_valid_idx,all_train_idx, all_model_files\n",
    "        modelTypeData['model_type'], modelTypeData['oof_pred'],modelTypeData['all_metrics'], modelTypeData['test_pred']=model_type, oof_pred, all_metrics, test_pred\n",
    "        with open(r\"kfold_models/\"+model_type.name()+\"_kfold.bin\", \"wb\") as output_file:\n",
    "            pickle.dump(modelTypeData, output_file)\n",
    "    def save_model(self, model_type, i_fold, model):\n",
    "        file = r\"models/\"+model_type.name()+\"_\"+str(i_fold)+\"_model\"\n",
    "        files = model_type.save(file, model)\n",
    "        files = self.tar(file, files)\n",
    "        print(\"saved model %s to %s (%s)\" % (model_type.name(), file, \", \".join(files)))\n",
    "        return files\n",
    "    def try_load_model(self, model_type, i_fold):\n",
    "        file = r\"models/\"+model_type.name()+\"_\"+str(i_fold)+\"_model\"\n",
    "        file = self.try_untar(file)\n",
    "        model, model_files = model_type.load(file)\n",
    "        if model is None:\n",
    "            print(\"could not load model %s from %s\" % (model_type.name(), file))\n",
    "        else:\n",
    "            print(\"loaded model %s from %s\" % (model_type.name(), file))\n",
    "        return model, model_files\n",
    "    def tar(self, file, files):\n",
    "        import tarfile\n",
    "        import os\n",
    "        tar = \"tar_\"+file + \".tar.gz.xyz\"\n",
    "        with tarfile.open(tar, \"w:gz\") as tara:\n",
    "            for f in files:\n",
    "                tara.add(f, arcname=os.path.basename(f))\n",
    "        files.append(tar)\n",
    "        return files\n",
    "    def try_untar(self, file):\n",
    "        import pathlib\n",
    "        import tensorflow as tf\n",
    "        import os\n",
    "        import tarfile\n",
    "        file_gz = \"tar_\"+file + \".tar.gz.xyz\"\n",
    "        filepath = pathlib.PurePath(file)\n",
    "        file_name = filepath.name\n",
    "        if not os.path.exists(file) and os.path.exists(file_gz):\n",
    "            import atexit, shutil, tempfile\n",
    "            models_dir = tempfile.mkdtemp()\n",
    "            atexit.register(shutil.rmtree, models_dir)\n",
    "            if not models_dir.endswith('/'):\n",
    "                models_dir = models_dir + '/'\n",
    "            print(\"loading from\", file_gz)\n",
    "            target_model_name = models_dir + file_name\n",
    "            with tarfile.open(file_gz) as my_tar:\n",
    "                my_tar.extractall(models_dir) # specify which folder to extract to\n",
    "                my_tar.close()\n",
    "            file = target_model_name\n",
    "            return file\n",
    "        return file\n",
    "    \n",
    "model_fold_storage = ModelFoldStorage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cf4571-97f1-4e9e-adf6-5d047c41865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyProblem:\n",
    "    def pred_dim(self):\n",
    "        return [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab60661-66eb-4b57-bcd3-bea7eb6e6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    def name(self):\n",
    "        pass\n",
    "    def create(self):\n",
    "        pass\n",
    "    def fit(self, model, Xt, yt, Xv, yv):\n",
    "        pass\n",
    "    def predict(self, model, Xt):\n",
    "        pass\n",
    "    def evaluate(self, model, x, y):\n",
    "        pass\n",
    "    def save(self, file, model):\n",
    "        pass\n",
    "    def load(self, file):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23db498-ecd7-4a79-a557-69c4c5c96752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DatasetMapFunction(input_ids, attn_masks, labels):\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attn_masks\n",
    "            }, labels\n",
    "class MyBertModel(MyModel):\n",
    "    def __init__(self, debug=False):\n",
    "        self.debug = debug\n",
    "    def create(self):\n",
    "        import tensorflow as tf\n",
    "        import tensorflow_hub as hub\n",
    "        import tensorflow_text as text\n",
    "        from transformers import TFBertModel\n",
    "        model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
    "        attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
    "\n",
    "        bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
    "        intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
    "        output_layer = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n",
    "\n",
    "        discourse_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "        discourse_model.summary()\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        discourse_model.compile(optimizer=Adam(learning_rate=1e-5, decay=1e-6), \n",
    "                        loss='categorical_crossentropy', \n",
    "                        metrics=['accuracy'])\n",
    "        return (discourse_model, self, model)\n",
    "    def name(self):\n",
    "        return \"bert\"\n",
    "    def fit(self, model, Xt, yt, Xv, yv):\n",
    "        X_input_ids, X_attn_masks, yt = self._transform(Xt, yt)\n",
    "        import tensorflow as tf\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, yt))\n",
    "        dataset = dataset.map(DatasetMapFunction)     # converting to required format for tensorflow dataset\n",
    "        dataset = dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor\n",
    "        X_val_input_ids, X_val_attn_masks, yv = self._transform(Xv, yv)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val_input_ids, X_val_attn_masks, yv))\n",
    "        val_dataset = val_dataset.map(DatasetMapFunction)     # converting to required format for tensorflow dataset\n",
    "        val_dataset = val_dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor\n",
    "        epochs = 5\n",
    "        if self.debug:\n",
    "            epochs=1\n",
    "        model[1].history = model[0].fit(dataset,\n",
    "            steps_per_epoch=200,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs)\n",
    "    def predict(self, model, X):\n",
    "        X_test_input_ids, X_test_attn_masks, _y = self._transform(X, None)\n",
    "        labels = model[0].predict([X_test_input_ids, X_test_attn_masks])\n",
    "        return labels\n",
    "    def evaluate(self, model, x, y):\n",
    "        import tensorflow as tf\n",
    "        X_input_ids, X_attn_masks, y = self._transform(x,y)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, y))\n",
    "        dataset = dataset.map(DatasetMapFunction)\n",
    "        dataset = dataset.shuffle(10000).batch(16, drop_remainder=True)\n",
    "        return model[0].evaluate(dataset, return_dict=True)\n",
    "    def save(self, file, model):\n",
    "        import shutil\n",
    "        import os\n",
    "        import tensorflow as tf\n",
    "        import pickle\n",
    "        ttt1=file\n",
    "        tf.keras.models.save_model(model[0], ttt1)\n",
    "        ttt2=file+\"_bert\"\n",
    "        tf.keras.models.save_model(model[2], ttt2)\n",
    "        ttt3=file+\"_python\"\n",
    "        with open(ttt3, \"wb\") as output_file:\n",
    "            pickle.dump(model[1].__dict__, output_file)\n",
    "        return [ttt1, ttt2, ttt3]\n",
    "    def load(self, file):\n",
    "        import os\n",
    "        import tensorflow as tf\n",
    "        import pickle\n",
    "        ttt1=file\n",
    "        ttt2=file+\"_bert\"\n",
    "        ttt3=file+\"_python\"\n",
    "        if not os.path.exists(ttt1) or not os.path.exists(ttt2) or not os.path.exists(ttt3):\n",
    "            return None, None\n",
    "        model_0 = tf.keras.models.load_model(ttt1)\n",
    "        model_2 = tf.keras.models.load_model(ttt2)\n",
    "        #with open(ttt3, \"rb\") as input_file:\n",
    "        #    model1_dict = pickle.load(input_file)\n",
    "        #self.__dict__.update(model1_dict)\n",
    "        model_files = [ttt1, ttt2, ttt3]\n",
    "        return (model_0, self, model_2), model_files\n",
    "    def _encode_data(self, df, ids, masks, tokenizer):\n",
    "        from tqdm.auto import tqdm\n",
    "        for i, text in tqdm(enumerate(df['text'])):\n",
    "            tokenized_text = tokenizer.encode_plus(\n",
    "                text,\n",
    "                max_length=256, \n",
    "                truncation=True, \n",
    "                padding='max_length', \n",
    "                add_special_tokens=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            ids[i, :] = tokenized_text.input_ids\n",
    "            masks[i, :] = tokenized_text.attention_mask\n",
    "        return ids, masks\n",
    "    \n",
    "    def _transform(self, X,y):\n",
    "        from transformers import BertTokenizerFast\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        X[\"text\"]=X['discourse_type'] + tokenizer.sep_token+ X['text']\n",
    "        if self.debug:\n",
    "            X = X.head(1000)\n",
    "            if y is not None:\n",
    "                y = y[0:1000,:]\n",
    "        \n",
    "        import numpy as np\n",
    "        X_input_ids = np.zeros((len(X), 256))\n",
    "        X_attn_masks = np.zeros((len(X), 256))\n",
    "        from tqdm.auto import tqdm\n",
    "        X_input_ids, X_attn_masks = self._encode_data(X, X_input_ids, X_attn_masks, tokenizer)\n",
    "        return X_input_ids, X_attn_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8024c639-2615-43a2-a66c-d29ab764b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackProblem(MyProblem):\n",
    "    def pred_dim(self):\n",
    "        return [0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce8e5d4-23aa-4d7c-8f53-966d0ed64381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', 2550)\n",
    "df_train = pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\n",
    "df_train[\"text\"] = df_train[\"essay_id\"].apply(lambda x: open(f'../input/feedback-prize-effectiveness/train/{x}.txt').read())\n",
    "df_test[\"text\"] = df_test[\"essay_id\"].apply(lambda x: open(f'../input/feedback-prize-effectiveness/test/{x}.txt').read())\n",
    "effectiveness_map = {\"Ineffective\":0, \"Adequate\":1,\"Effective\":2}\n",
    "df_train[\"target\"] = df_train[\"discourse_effectiveness\"].map(effectiveness_map)\n",
    "labels = np.zeros((len(df_train), 3))\n",
    "labels[np.arange(len(df_train)), df_train['target'].values] = 1\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1961fd-1c66-424c-ac8a-f8190d345105",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_type = FeedbackProblem()\n",
    "model_type = MyBertModel(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf7c83d-c830-4ebf-8587-d969517713eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfold 6248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 1000.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "loaded model bert from models/bert_0_model\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vladimir\\AppData\\Local\\Temp/ipykernel_6248/899260264.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[\"text\"]=X['discourse_type'] + tokenizer.sep_token+ X['text']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d988e6ab8f2489d8cbfca1081aec2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 [==============================] - 28s 115ms/step\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68208cc2e7b34bdcb20c309f82d56a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459/459 [==============================] - 28s 58ms/step - loss: 0.7114 - accuracy: 0.6800\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffd93c843ab453b9ee7651283657446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 310ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "loaded model bert from models/bert_1_model\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd1b19d6ac54ec2b34b1dd3252a7b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 [==============================] - 27s 115ms/step\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1621d4e528c3473a8b8fd63e7186c55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459/459 [==============================] - 28s 58ms/step - loss: 0.7080 - accuracy: 0.6770\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b202c86bb9a34d208c7d78e07689e4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 334ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "loaded model bert from models/bert_2_model\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ce2e64ec7545d48b9e228b4340f1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 [==============================] - 27s 116ms/step\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f924328afd450681d03bec9b3b365c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459/459 [==============================] - 28s 58ms/step - loss: 0.7789 - accuracy: 0.6556\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281e5dd1bec9476aaec69ba658cad071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 196ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "loaded model bert from models/bert_3_model\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c985ff45854e4c97b99dd752a133a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 [==============================] - 27s 115ms/step\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccce38c4e0d84651b172f2dd408229bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459/459 [==============================] - 28s 58ms/step - loss: 0.7046 - accuracy: 0.6857\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b0292d436643dcb9eb4fa70a196cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 186ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "loaded model bert from models/bert_4_model\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3edc13fa234afcb3b6c89c56979b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 [==============================] - 28s 116ms/step\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9be763a9c944c548520da2c6a4056ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459/459 [==============================] - 28s 58ms/step - loss: 0.7097 - accuracy: 0.6830\n",
      "['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'flax_model.msgpack', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d8e3b7894d43cda8513d098dc85a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 188ms/step\n"
     ]
    }
   ],
   "source": [
    "oof_pred, test_pred, all_metrics=kfold_model_inproc(model_fold_storage, problem_type, model_type, df_train, labels, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e78291d-5ef1-4a8e-892a-f88ddeb0701a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9303f89d-91b7-418a-8443-0d2ae12013c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = np.mean(test_pred, axis=0)\n",
    "pred_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9047398-0053-4328-8e4b-b020f7131418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a261b6e14276</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a88900e7dc1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9790d835736b</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75ce6d68b67b</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93578d946723</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id  Ineffective  Adequate  Effective\n",
       "0  a261b6e14276         0.20      0.60       0.40\n",
       "1  5a88900e7dc1         3.00      6.00       1.00\n",
       "2  9790d835736b         1.00      2.00       3.00\n",
       "3  75ce6d68b67b         0.33      0.34       0.33\n",
       "4  93578d946723         0.01      0.24       0.47"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('../input/feedback-prize-effectiveness/sample_submission.csv')\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d28522-22bc-4cd1-8a1c-7f3721c0f6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4b15325-5cb8-4a64-94b3-d60d486af151",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6248/1154407178.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'discourse_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'discourse_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msample_submission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Ineffective'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Adequate'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Effective'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submission.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_labels' is not defined"
     ]
    }
   ],
   "source": [
    "sample_submission['discourse_id'] = df_test['discourse_id']\n",
    "sample_submission['Ineffective'] = pred_labels[:,0]\n",
    "sample_submission['Adequate'] = pred_labels[:,1]\n",
    "sample_submission['Effective'] = pred_labels[:,2]\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587dd81e-441c-4839-b96d-71f72b40792e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
